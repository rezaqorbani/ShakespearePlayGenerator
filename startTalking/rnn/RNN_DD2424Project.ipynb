{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4oQPpY3hRYz",
        "outputId": "c316ba62-1756-48ad-c3f2-e37f22ffde58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping loaded\n",
            "2623471\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from feature_engineering import DataProcessor\n",
        "from LSTM import LSTMModel\n",
        "from RNN import RNNModel\n",
        "from trainRNN import ModelTrainer\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 100\n",
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load and preprocess data\n",
        "dataset_dir = ''\n",
        "data_processor = DataProcessor(dataset_dir)\n",
        "\n",
        "# Choose to preprocess data or load saved mappings\n",
        "preprocess_data = False\n",
        "char_to_id_file = 'char_to_id.json'\n",
        "id_to_char_file = 'id_to_char.json'\n",
        "\n",
        "if preprocess_data:\n",
        "    ids, char_to_id, id_to_char = data_processor.preprocess()\n",
        "    data_processor.save_mappings(char_to_id_file, char_to_id, id_to_char_file, id_to_char)\n",
        "else:\n",
        "    char_to_id, id_to_char = data_processor.load_mappings(char_to_id_file, id_to_char_file)\n",
        "print('Mapping loaded')\n",
        "## Create Dataset sequences with pytorch\n",
        "#data_processor = DataProcessor(dataset_dir)\n",
        "dialogue_lines = data_processor.read_dialogue_lines()\n",
        "text = ' '.join(dialogue_lines.values())\n",
        "ids = data_processor.text_to_ids(text, char_to_id)\n",
        "print(len(ids))\n",
        "dataset = data_processor.create_dataset(ids[:1000050])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_BsL7luYqzM",
        "outputId": "240814cc-1b40-4caa-eccc-1b0da242a75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trainRNN import ModelTrainer\n",
        "\n",
        "train_loader, val_loader, test_loader=DataProcessor.create_loaders(dataset, 0.8, 0.1, 0.1, 50)\n",
        "print('Data Loaded')\n",
        "## Train and Evaluate the model\n",
        "# Initialize the RNN model\n",
        "input_size = embedding_size = 50\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "vocab_size = len(char_to_id)\n",
        "model = RNNModel(input_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function, learning rate, and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "trainer=ModelTrainer(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "print('start training')\n",
        "# Training loop\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(epoch)\n",
        "    loss = trainer.train()\n",
        "    # Evaluate the model on the validation set\n",
        "    validation_loss = trainer.evaluate(val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n",
        "\n",
        "# Evaluation on test set\n",
        "test_loss = trainer.evaluate(test_loader)\n"
      ],
      "metadata": {
        "id": "Bq9MTBhiu63x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5508a7b-6d2b-4d6b-e853-7393662cffe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Loaded\n",
            "start training\n",
            "0\n",
            "Epoch 1/10, Loss: 1.3664, Validation Loss: 1.3453\n",
            "1\n",
            "Epoch 2/10, Loss: 1.2337, Validation Loss: 1.3267\n",
            "2\n",
            "Epoch 3/10, Loss: 1.2109, Validation Loss: 1.3207\n",
            "3\n",
            "Epoch 4/10, Loss: 1.2016, Validation Loss: 1.3184\n",
            "4\n",
            "Epoch 5/10, Loss: 1.1973, Validation Loss: 1.3149\n",
            "5\n",
            "Epoch 6/10, Loss: 1.1955, Validation Loss: 1.3156\n",
            "6\n",
            "Epoch 7/10, Loss: 1.1946, Validation Loss: 1.3147\n",
            "7\n",
            "Epoch 8/10, Loss: 1.1940, Validation Loss: 1.3168\n",
            "8\n",
            "Epoch 9/10, Loss: 1.1941, Validation Loss: 1.3160\n",
            "9\n",
            "Epoch 10/10, Loss: 1.1943, Validation Loss: 1.3183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loss on testing data:', test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK_86fdefG6T",
        "outputId": "44fc4522-d061-4699-b9d6-41342f28674c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on testing data: 1.318735296010971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(trainer.model, 'rnn_model')"
      ],
      "metadata": {
        "id": "DEoK2FonsIDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=trainer.model\n"
      ],
      "metadata": {
        "id": "_zQi-VYkf2gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluateRNN import Evaluater\n",
        "# Evaluate performance\n",
        "evaluater = Evaluater(model, device)\n",
        "perplexity= evaluater.calculate_perplexity(test_loader, criterion)\n",
        "print('Perplexity:' , perplexity)"
      ],
      "metadata": {
        "id": "AtHZxOows-aY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba87bc6-d940-4438-c9c2-59c414baf62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 3.7386900498074134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Start a discussion about coffee\"\n",
        "gen_length = 2000\n",
        "temperature = 0.8\n",
        "\n",
        "generated_text = evaluater.generate_text(seed_text, gen_length, char_to_id, id_to_char, device, temperature)\n",
        "#generated_text = generate_text(model, seed_text, gen_length, char_to_id, id_to_char, device, temperature)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri3yaJ3rCvUY",
        "outputId": "b71aef98-8d79-40ba-a523-e34a30adacd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start a discussion about coffee. The sun ever supposed to know it, that is, Madame.  You have no problem. There must be mad. Better than to find out about it. You have a few linetion? I wanted your dreams in the groovy. I'd shot to believe you and yours for what you're doing, Sarah Lawrence you pay our different man. I bet. Well... no. I am sorry, Milo? A why you're true to a helluva girls and the Indies war with his meetended.  And me are you doing?!  He's bleing Daphool. You would have head going in the local place you down to fix cass you say it anything out a man wonderful! Okay. May not at lash, don't believe you.  If I took the dirty safe...  But it's ones to go out was the boached. Good\". Well, what? No connection my damage private you guys six, Curke.  It's about to be as a nicked by any patives up. If they were so sorry. You mean I'mmore when you go holding Promancesshand close to the only think about Loodon Airvolves sir. You have you came out of your bachelor party well. The world and million? Well, I feel anyway? Time if I can't tell! Milo, she was dealing hundred dispose in Mercuctions come up with me? Yes. This way! This was accident. Bad as I right? Sorry. Please. I preful? No. It's building. Or slaperation good floor? Oh yes, I promise young ask me. All the nobles - for that if you will be okay. Okay. What? I'll tell you how you in death, and you might be backwit. You booking, that's why they can only neither and a press on your attention with laser. You see that? Yes. Oh, I don't like to put this academy. Whiteh? What?! Walks. So what's going on? Fallate. No, where are you doing? I know. Put three. ...Moon? All right, Davis.  I have to be late with since I am sorry, Bielder. But don't want to catch the company very small. You think that's an ass. Why not afficide. I don't answer my head mile. For now, but the right arm now? You don't like it? You will be happy. No one knows what you mean it's not a barticulor and waitin' to say. I've been in Brian. My father's wait department. I \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, seed_text, gen_length, char_to_id, id_to_char, device, temperature=1.0):\n",
        "        model.eval()\n",
        "\n",
        "        # Convert seed_text to tensor\n",
        "        input_seq = [char_to_id[char] for char in seed_text]\n",
        "        input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)  # Add batch_first dimension\n",
        "\n",
        "        # Initialize the hidden state\n",
        "        hidden = model.init_hidden(1)\n",
        "\n",
        "        # Generate text\n",
        "        generated_text = seed_text\n",
        "        for _ in range(gen_length):\n",
        "            with torch.no_grad():\n",
        "                outputs, hidden = model(input_seq, hidden)\n",
        "                char_probs = F.softmax(outputs[-1, :] / temperature, dim=0)\n",
        "\n",
        "                # Sample a character from the output probabilities\n",
        "                char_idx = torch.multinomial(char_probs, 1).item()\n",
        "\n",
        "                # Append the generated character to the generated text\n",
        "                generated_char = id_to_char[str(char_idx)]\n",
        "                generated_text += generated_char\n",
        "\n",
        "                # Update the input sequence with the generated character\n",
        "                input_seq = torch.tensor([[char_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "        return generated_text"
      ],
      "metadata": {
        "id": "4d7qzlmogm3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXVMFdJuhHTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}