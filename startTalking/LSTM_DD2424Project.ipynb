{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4oQPpY3hRYz",
        "outputId": "7f67552e-9140-4051-eefb-3cda60205314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping loaded\n",
            "17146310\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from feature_engineering import DataProcessor\n",
        "from LSTM import LSTMModel\n",
        "from train import ModelTrainer\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 100\n",
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load and preprocess data\n",
        "dataset_dir = '/content/data/'\n",
        "data_processor = DataProcessor(dataset_dir)\n",
        "\n",
        "# Choose to preprocess data or load saved mappings\n",
        "preprocess_data = False\n",
        "char_to_id_file = 'char_to_id.json'\n",
        "id_to_char_file = 'id_to_char.json'\n",
        "\n",
        "if preprocess_data:\n",
        "    ids, char_to_id, id_to_char = data_processor.preprocess()\n",
        "    data_processor.save_mappings(char_to_id_file, char_to_id, id_to_char_file, id_to_char)\n",
        "else:\n",
        "    char_to_id, id_to_char = data_processor.load_mappings(char_to_id_file, id_to_char_file)\n",
        "print('Mapping loaded')\n",
        "## Create Dataset sequences with pytorch\n",
        "#data_processor = DataProcessor(dataset_dir)\n",
        "dialogue_lines = data_processor.read_dialogue_lines()\n",
        "text = ' '.join(dialogue_lines.values())\n",
        "ids = data_processor.text_to_ids(text, char_to_id)\n",
        "print(len(ids))\n",
        "dataset = data_processor.create_dataset(ids[:1000050])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_BsL7luYqzM",
        "outputId": "5e1f55b8-1b9c-4d53-bf3e-03e6832007bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from train import ModelTrainer\n",
        "\n",
        "train_loader, val_loader, test_loader=DataProcessor.create_loaders(dataset, 0.8, 0.1, 0.1, 50)\n",
        "print('Data Loaded')\n",
        "## Train and Evaluate the model\n",
        "# Initialize the LSTM model\n",
        "input_size = embedding_size = 50\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "vocab_size = len(char_to_id)\n",
        "model = LSTMModel(input_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function, learning rate, and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "trainer=ModelTrainer(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "print('start training')\n",
        "# Training loop\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(epoch)\n",
        "    loss = trainer.train()\n",
        "    # Evaluate the model on the validation set\n",
        "    validation_loss = trainer.evaluate(val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n",
        "\n",
        "# Evaluation on test set\n",
        "test_loss = trainer.evaluate(test_loader)\n"
      ],
      "metadata": {
        "id": "Bq9MTBhiu63x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5414b9-c4ff-4364-84d9-ab4ca72309e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Loaded\n",
            "start training\n",
            "0\n",
            "Epoch 1/10, Loss: 1.4919, Validation Loss: 1.3284\n",
            "1\n",
            "Epoch 2/10, Loss: 1.2815, Validation Loss: 1.2465\n",
            "2\n",
            "Epoch 3/10, Loss: 1.2167, Validation Loss: 1.1975\n",
            "3\n",
            "Epoch 4/10, Loss: 1.1737, Validation Loss: 1.1590\n",
            "4\n",
            "Epoch 5/10, Loss: 1.1430, Validation Loss: 1.1331\n",
            "5\n",
            "Epoch 6/10, Loss: 1.1204, Validation Loss: 1.1170\n",
            "6\n",
            "Epoch 7/10, Loss: 1.1038, Validation Loss: 1.1024\n",
            "7\n",
            "Epoch 8/10, Loss: 1.0910, Validation Loss: 1.0909\n",
            "8\n",
            "Epoch 9/10, Loss: 1.0812, Validation Loss: 1.0838\n",
            "9\n",
            "Epoch 10/10, Loss: 1.0734, Validation Loss: 1.0776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loss on testing data:', test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK_86fdefG6T",
        "outputId": "adb5af71-8c69-4ca9-c98f-b38bf08c1e80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on testing data: 1.0789489044249059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from LSTM import LSTMModel"
      ],
      "metadata": {
        "id": "TgptYlnAfepk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "#trainer.model.save_model('lstm_2layers_10epochs')\n",
        "torch.save(trainer.model, 'lstm2layers_10epochs')"
      ],
      "metadata": {
        "id": "DEoK2FonsIDi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=trainer.model\n"
      ],
      "metadata": {
        "id": "_zQi-VYkf2gz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import Evaluater\n",
        "# Evaluate performance\n",
        "evaluater = Evaluater(model, device)\n",
        "perplexity= evaluater.calculate_perplexity(test_loader, criterion)\n",
        "print('Perplexity:' , perplexity)"
      ],
      "metadata": {
        "id": "AtHZxOows-aY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f69b27-6211-4f06-b93f-22ee67f308e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 2.9415860374933525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Start a discussion about coffee\"\n",
        "gen_length = 2000\n",
        "temperature = 0.8\n",
        "\n",
        "#generated_text = evaluater.generate_text(seed_text, gen_length, char_to_id, id_to_char, device, temperature)\n",
        "generated_text = generate_text(model, seed_text, gen_length, char_to_id, id_to_char, device, temperature)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri3yaJ3rCvUY",
        "outputId": "55c933b5-0126-4823-dd81-7fa169aaf770"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start a discussion about coffee this time, Coset So and the prick for this point. What did you have no fantasy regarding the bathroom with your finds out.  Let's talk to you. You know what you mean to me. I don't be so bad as an itty thousand days. How many. What you are the one of Seah? But first, or freewilut in the arrangements. Thank you. The first thing was on his pardon? Are you gonna tell me about him. He's damn it. What about some kind of guy.  He's not finished - not so...  ...did you know? No. Baid who do you want to see Mr. Dallas. So you didn't shoot West. He did it is, ashamed. I have a finger out of here, chamber if it gets all week. I don't know... Emmoner, Dave to use a failure you going? How can you be so kind. All right, little being out there. What did you say, Carner. C'mon, we're going in this? They think they didn't work at the contemand right away? Now one. So her manualive... This does not we?  I may can't. I don't know. I was crazy, I really didn't like him by the Senator's well, see a lot here. Those gal a piece of it. Come on, no Colo, Care, now, honey. I'm pissed out what was your departmental emergency and see the one thought, pal from the apartment for your heads, the buried in two of juck and yet. Where's the money. Right. Love is planning to see -- Beet you something, I can't help my daughter be the problems and wear one such a girl. I know, I work up a better than your sleep. Ahh, find them in the walls. Yeah. He has taking her to my parts. I wanted to go home. That's the great fair to Crained that could be more days were you holding. It's fucking a 747 empty. A fellow. I'm fucking on the character.  I'm sure... You were supposed to buy bread the now I'm happy.  You got a welfare it. Look at your visit you guys are microwanna don't know... Hey! Well dies, we're lose. How'd I ever say hello... This is an emergency.  They think I have some things about Paul Owen. We have to do with you something Luther.  You know what I think it's been line she can be in life.  Good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, seed_text, gen_length, char_to_id, id_to_char, device, temperature=1.0):\n",
        "        model.eval()\n",
        "\n",
        "        # Convert seed_text to tensor\n",
        "        input_seq = [char_to_id[char] for char in seed_text]\n",
        "        input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)  # Add batch_first dimension\n",
        "\n",
        "        # Initialize the hidden state\n",
        "        hidden = model.init_hidden(1)\n",
        "\n",
        "        # Generate text\n",
        "        generated_text = seed_text\n",
        "        for _ in range(gen_length):\n",
        "            with torch.no_grad():\n",
        "                outputs, hidden = model(input_seq, hidden)\n",
        "                char_probs = F.softmax(outputs[-1, :] / temperature, dim=0)\n",
        "\n",
        "                # Sample a character from the output probabilities\n",
        "                char_idx = torch.multinomial(char_probs, 1).item()\n",
        "\n",
        "                # Append the generated character to the generated text\n",
        "                generated_char = id_to_char[str(char_idx)]\n",
        "                generated_text += generated_char\n",
        "\n",
        "                # Update the input sequence with the generated character\n",
        "                input_seq = torch.tensor([[char_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "        return generated_text"
      ],
      "metadata": {
        "id": "4d7qzlmogm3g"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXVMFdJuhHTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}